#!/usr/bin/env python3
"""DCInside ê²Œì‹œë¬¼ì„ ê°€ì ¸ì™€ ì €ì¥í•˜ê³  Codex CLIë¡œ ìš”ì•½í•©ë‹ˆë‹¤."""

from __future__ import annotations

import os
from pathlib import Path
from typing import List

import psycopg2
import requests

from crawl_dcinside import HEADERS, Post, TARGET_URL, fetch_posts
from codex_summary import CodexConfig, SummaryError, summarise_with_codex
from content_fetcher import contains_video_url, download_images, fetch_post_body
from db_utils import (
    SourceConfig,
    ensure_tables,
    get_or_create_source,
    replace_item_assets,
    replace_item_comments,
    delete_item,
    seed_sources_from_file,
    upsert_items,
    update_item_with_summary,
)


def load_env_file(path: Path = Path(".env")) -> None:
    """.env íŒŒì¼ì˜ í‚¤ì™€ ê°’ì„ ì½ì–´ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."""
    if not path.exists():
        return

    try:
        with path.open("r", encoding="utf-8") as env_file:
            for raw_line in env_file:
                line = raw_line.strip()
                if not line or line.startswith("#"):
                    continue
                if "=" not in line:
                    continue
                key, _, value = line.partition("=")
                key = key.strip()
                if not key or key in os.environ:
                    continue
                cleaned_value = value.strip()
                if (
                    len(cleaned_value) >= 2
                    and cleaned_value[0] == cleaned_value[-1]
                    and cleaned_value[0] in {'"', "'"}
                ):
                    cleaned_value = cleaned_value[1:-1]
                os.environ[key] = cleaned_value
    except OSError:
        pass


load_env_file()


def getenv_casefold(key: str) -> str | None:
    """í™˜ê²½ ë³€ìˆ˜ í‚¤ì˜ ëŒ€ì†Œë¬¸ìì™€ ìƒê´€ì—†ì´ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    target = key.casefold()
    for env_key, value in os.environ.items():
        if env_key.casefold() == target:
            return value
    return None


def env_flag(key: str, default: bool = False) -> bool:
    """ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•Šê³  í™˜ê²½ ë³€ìˆ˜ ê°’ì„ ë¶ˆë¦¬ì–¸ìœ¼ë¡œ í•´ì„í•©ë‹ˆë‹¤."""
    value = getenv_casefold(key)
    if value is None:
        return default
    return value.strip().lower() not in {"", "0", "false", "off", "no"}


def env_int(key: str, default: int) -> int:
    """ì •ìˆ˜í˜• í™˜ê²½ ë³€ìˆ˜ ê°’ì„ ì½ê³  ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    value = getenv_casefold(key)
    if value is None:
        return default
    try:
        return int(value.strip())
    except (TypeError, ValueError):
        return default


DB_CONFIG = {
    "dbname": "discordbot",
    "user": "hsh",
    "password": "",
    "host": "localhost",
    "port": 5432,
}

SOURCE_CONFIG = SourceConfig(
    code="dcinside_thesingularity_recommend",
    name="DCInside íŠ¹ì´ì  ì¶”ì²œ",
    url_pattern=(
        "https://gall.dcinside.com/mgallery/board/view/?id=thesingularity&no={external_id}"
        "&exception_mode=recommend&page={page}"
    ),
    parser="dcinside_recommend_v1",
    fetch_interval_minutes=60,
    metadata={
        "board_id": "thesingularity",
        "exception_mode": "recommend",
        "target_url": TARGET_URL,
    },
)

ALLOWED_SUBJECTS = {
    "ì¼ë°˜",
    "ì •ë³´/ë‰´ìŠ¤",
    "ğŸ†ë² ìŠ¤íŠ¸",
    "ì‚¬ìš©í›„ê¸°",
    "AIí™œìš©",
    "ìë£Œì‹¤",
    "ì—­ë…¸í™”",
    "í† ì˜",
    "ëŒ€íšŒ",
}

ASSET_ROOT = Path("data/assets")
ASSET_ROOT.mkdir(parents=True, exist_ok=True)

CODEX_DEBUG = env_flag("CODEX_DEBUG")
CODEX_MODEL = getenv_casefold("CODEX_MODEL") or "gpt-5-codex"
CODEX_TIMEOUT_SECONDS = env_int("CODEX_TIMEOUT", 300)
MAX_TEXT_FOR_SUMMARY = 4000


def _comment_lines_for_summary(comments: List[dict]) -> List[str]:
    """ëŒ“ê¸€ ì „ì²´ë¥¼ êµ¬ì¡° ì •ë³´ì™€ í•¨ê»˜ ìš”ì•½ ì…ë ¥ì— í¬í•¨í•  ìˆ˜ ìˆë„ë¡ ë¬¸ìì—´ë¡œ ë§Œë“ ë‹¤."""
    if not comments:
        return []

    id_to_author = {
        str(comment.get("external_id")): (comment.get("author") or "unknown")
        for comment in comments
        if comment.get("external_id") is not None
    }

    lines: List[str] = []
    for comment in comments:
        if not isinstance(comment, dict):
            continue
        author = comment.get("author") or "unknown"
        content = (comment.get("content") or "").strip()
        if not content:
            continue

        metadata = comment.get("metadata") or {}
        depth = metadata.get("depth")
        try:
            depth_level = int(depth) if depth is not None else 0
        except (TypeError, ValueError):
            depth_level = 0

        parent_external = comment.get("parent_external_id")
        parent_author = None
        if parent_external is not None:
            parent_author = id_to_author.get(str(parent_external))

        indent = "  " * max(depth_level, 0)
        if depth_level <= 0:
            label = "[ì›ëŒ“ê¸€]"
        else:
            label = f"[ëŒ€ëŒ“ê¸€ â†’ {parent_author}]" if parent_author else "[ëŒ€ëŒ“ê¸€]"

        line = f"{indent}{label} {author}: {content}"
        lines.append(line)

    return lines


def process_details(
    conn,
    jobs,
    codex_config: CodexConfig,
    asset_root: Path = ASSET_ROOT,
) -> None:
    """ìƒì„¸ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ê³  ì—ì…‹ì„ ê´€ë¦¬í•˜ë©° ì½˜í…ì¸ ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
    for post, item_id, _inserted in jobs:
        try:
            body_text, image_urls, comments = fetch_post_body(post.url, HEADERS)
        except requests.RequestException as exc:
            body_text, image_urls, comments = "", [], []
            last_error = f"Detail fetch failed: {exc}"
            update_item_with_summary(
                conn,
                item_id,
                summary=None,
                raw_text=body_text,
                image_count=0,
                model_name=codex_config.model,
                last_error=last_error,
            )
            continue

        if contains_video_url(image_urls):
            print(f"Skipping video post {post.url}; deleting item {item_id}.")
            delete_item(conn, item_id)
            continue

        replace_item_comments(conn, item_id, comments)
        summary_input = body_text
        comment_lines = _comment_lines_for_summary(comments)
        if comment_lines:
            summary_input = (
                summary_input
                + "\n\nëŒ“ê¸€ ì „ì²´ ëª©ë¡ (ì›ëŒ“ê¸€/ëŒ€ëŒ“ê¸€ êµ¬ì¡°):\n"
                + "\n".join(comment_lines)
            )

        assets = download_images(
            image_urls=image_urls,
            external_id=post.external_id or str(item_id),
            referer=post.url,
            asset_root=asset_root,
            headers=HEADERS,
        )
        replace_item_assets(conn, item_id, assets)

        image_paths = [asset["local_path"] for asset in assets]
        last_error = None
        summary = None
        try:
            summary = summarise_with_codex(summary_input, image_paths, codex_config)
        except SummaryError as exc:
            last_error = str(exc)
            summary = (
                summary_input[: codex_config.max_text_length] if summary_input else None
            )

        update_item_with_summary(
            conn,
            item_id,
            summary,
            body_text,
            image_count=len(image_paths),
            model_name=codex_config.model,
            last_error=last_error,
        )


def main() -> None:
    posts = [post for post in fetch_posts() if post.subject in ALLOWED_SUBJECTS]
    if not posts:
        print("No posts matched allowed subjects; aborting.")
        return

    codex_config = CodexConfig(
        model=CODEX_MODEL,
        timeout_seconds=CODEX_TIMEOUT_SECONDS,
        max_text_length=MAX_TEXT_FOR_SUMMARY,
        debug=CODEX_DEBUG,
    )

    with psycopg2.connect(**DB_CONFIG) as conn:
        ensure_tables(conn)
        seed_sources_from_file(conn, Path("config/sources.json"))
        source, created = get_or_create_source(conn, SOURCE_CONFIG)
        if created:
            print(
                "Created source configuration with is_active=FALSE. "
                "Update the record to enable crawling."
            )

        if not source.get("is_active"):
            print(f"Source '{source.get('code')}' is inactive; skipping crawl.")
            return

        jobs = upsert_items(conn, source["id"], posts)
        process_details(conn, jobs, codex_config)

    print(f"Processed {len(jobs)} posts (details fetched & summarised).")


if __name__ == "__main__":
    main()
